# Chapter 15 - 공간 기반 아키텍처 스타일



## 토폴로지

웹 기반 비지니스 애플리케이션은 대부분 일반적인 요청 흐름을 따라간다. 하지만 유저가 늘어날수록 병목 현상이 나타나고 병목 현상이 생긴 구간을 확장해서 해결한다해도 연쇄 반응으로 다른 구간에서 병목현상이 생길 것이다.

따라서 공간 기반 아키텍처 스타일을 사용해 위와 같은 문제를 해결할 수 있다. 특히 가변적인 상황이라 예측이 불가능한 애플리케이션에서 공간 기반 아키텍처 스타일은 유용하다.

공간 기반 아키텍처라는 명칭은 튜플에서 유래되었다. 튜플 공간은 공유 메모리를 통해 통신하는 다중 병렬 프로세서를 사용하는 기술이다. 즉 **시스템 동기 제약조건인 중앙 데이터베이스를 없애는 대신, 복제된 인메모리 데이터 그리드를 활용**하면 확장성, 탄력성, 성능을 높일 수 있다.



### 1. 처리 장치

공간 기반 아키텍처 스타일의 구조는 5가지가 있는데 그중 첫번째가 처리 장치이다. 

처리 장치는 애플리케이션 코드를 가지고 있어, 보통 웹 기반 컴포넌트와 백엔드 비즈니스 로직이 포함되나 애플리케이션 종류마다 내용물이 달라진다. 작은 웹 기반은 단일로 대규모 애플리케이션은 기능별로 나누어 배포해야 한다.

### 2. 가상 미들웨어

두번째로 가상 미들웨어이다. 

아키텍처 내부에서 데이터 동기화 및 요청 처리의 다양한 부분을 제어 및 관리 조정하는 인프라를 담당한다. 가상 미들웨어는 3가지등의 컴포넌트로 구성되는데 다음과 같다.

1. 메시징 그리드
   * 입력 요청과 세션 상태를 관리하는 컴포넌트
   * 가상 미들웨어에 요청이 유입되면 어느 활성 처리 장치가 요청을 받아 처리할지 결정하여 해당 처리 장치로 요청 전달
   * 복잡도는 다양하게 사용 ( 라운드로빈부터 요청 처리 상태를 추적하는 복잡한 알고리즘까지 등 )
   * 보통 부하 분산이 가능한 일반 웹 서버로 구성 (Nginx, HA proxy)
2. 데이터 그리드
   * 데이터를 동기화 하고 처리하는 필수 컴포넌트
   * 필요에 따라 처리 장치만 이든지 혹은 처리장치, 가상 미들웨어 둘다 위치 (외부 컨트롤러가 필요한 복제 캐시나 분산 캐시를 사용할 경우)
   * **각 처리 장치에서 가지고 있는 인메모리 데이터 그리드**들은 정확히 동일한 데이터를 갖고 있어야 함
   * 비동기 업데이트를 통해 데이터를 동기화 시키고 인스턴스가 하나 다운되더라도 데이터들을 유지시킴
3. 처리 그리드
   * 필수 컴포넌트는 아니지만, 다수의 처리 장치가 단일 비즈니스 요청을 처리할 경우 요청 처리를 오케스트레이션하는 일을 담당하는 컴포넌트
   * 종류가 다른 처리장치 사이에 조정이 필요한 요청이 들어오면 중재 및 조정
4. 배포 관리자
   * 부하 조건에 따라 처리 장치 인스턴스를 동적으로 시작 및 종료하는 컴포넌트
   * 응답 시간, 유저 부하를 계속 모니터링 하면서 새로운 처리 장치를 기동하고 종료

### 3. 데이터 펌프

세번째는 데이터 펌프로 데이터를 다른 프로세서에 보내 데이터 베이스를 업데이트하는 장치이다.

처리 장치는 직접 데이터베이스에서 읽고 쓰지 않기 때문에 메시징 기법으로 데이터를 데이터베이스에 전달한다. 즉 항상 비동기로 동작하면서 메모리 캐시와 데이터베이스의 최종 일관성을 실현시키다.

대부분의 데이터 펌프는 도메인이나 그 서브도메인 별로 여러 개를 사용한다. 그리고 데이터 펌프는 계약 데이터와 연관된 액션을 포함한다. 계약 포맷은 JSON 스키마, XML 스키마, 객체, 값 기반 메시지 등 다양하다. 그리고 업데이트 데이터는 보통 데이터 펌프 안에 새 데이터 값만 보관한다.

### 4. 데이터 라이터

네번째는 데이터 라이터인데 데이터 펌프에서 메시지를 받아 그에 맞게 데이터베이스를 업데이트 하는 컴포넌트이다.

데이터라이터는 서비스나 애플리케이션, 데이터 허브로 구현할 수 있고, 데이터 펌프나 처리 장치에 따라 데이터 라이터의 세분도를 나눌 수 있다. 가령 모든 도메인의 데이터 펌프를 리스닝 하거나, 각 데이터 펌프 마다 데이터 라이터를 구성할 수 있다. 이는 각각 장단점이 있으며 확장성과 민첩성, 복잡성의 차이를 둘 수 있다.

### 5. 데이터 리더

마지막으로 데이터 리더이다. 데이터 리더는 데이터베이스에서 데이터를 읽어 리버스 데이터 펌프를 통해 처리 장치로 실어 나르는 컴포넌트이다. 

데이터 리더는 세가지 경우에만 작동되는데 다음가 같다.

1. 동일한 이름의 캐시를 가진 모든 처리 장치 인스턴스가 실패한 경우
2. 동일한 이름의 캐시 안에서 모든 처리 장치를 재배포한 경우
3. 복제 캐시에 들어있지 않은 아카이브 데이터를 조회하는 경우

데이터 리더도 데이터 라이터처럼 도메인 기반으로 할 수 있지만 특정 장치의 클래스 전용으로 사용하는게 보통이다. 그리고 서비스나 애플리케이션, 데이터 허브로 구현할 수 있다.

데이터 라이터와 리더는 본질적으로 데이터 액세스 레이어를 형성한다. 하지만 이 둘의 차이점은 처리 장치가 데이터베이스의 테이블 혹은 스키마 구조를 얼마나 잘 아느냐이다. 

데이터 액세스 레이어는 처리 장치가 데이터베이스의 하부 데이터 구조와 커플링 되어 있으므로 데이터 리더, 라이터만 사용해서 간접적으로 데이터베이스에 액세스한다.



## 데이터 충돌

이름이 동일한 캐시가 포함된 서비스 인스턴스에 시시각각 업데이트가 일어나는 상태에서 복제 캐시를 사용하면 **복제 레이턴시**가 발생해 **데이터 충돌**이 일어 날 수 있다. 

즉 한 캐시 인스턴스에서 데이터가 업데이트 되어 다른 캐시 인스턴스에 복제하는 도중 동일한 데이터가 해당 캐시에서 업데이트 되는 현상을 말한다. ( 데이터 불일치 )

데이터 충돌 발생 빈도는 수학적 계산으로 공식을 도출할 수 있다. 

> 충돌률 = N * UR^2 / S * RL
>
> 1. N : 동일한 이름의 캐시를 사용하는 인스턴스 수
> 2. UR : 밀리초 당 업데이트 율
> 3. S : 캐시 크기
> 4. RL : 캐시 제품의 복제 대기 시간

예를 들면 다음과 같다.

1. 업데이트율 (UR) : 20 업데이트 / 초
2. 인스턴스 수 (N) : 5
3. 캐시 크기 (S) : 50000로우
4. 복제 레이턴시 (RL) : 100밀리초
5. **충돌률 : 시간당 14.4**
6. 업데이트 : 시간당 72000
7. 비율 : 0.02%

이걸 풀어쓰면 시간당 72000개의 업데이트가 발생하고 그중 14개 업데이트가 충돌할 가능성이 있다는 뜻이다. 즉 0.02% 이므로 복제 캐시는 나쁘지 않다는 뜻이다.

가변적인 복제 레이턴시는 데이터 일관성에 중대한 영향을 미치기 때문에 직접 프로덕션 환경에서 측정해봐야 한다. 또한 저 식에서 캐시 크기만 값이 클수록 충돌률이 작아지기 때문에 알아두면 좋다.



## 클라우드 대 온프레미스 구현

공간 기반 아키텍처의 가장 큰 장점은 클라우드와 온프레미스를 같이 하이브리드 형태로 구성할 수 있다는 것이다. 이러한 이유는 비동기 데이터 펌프와 이 아키텍처 스타일의 최종 일관성 모델 덕분이기 때문이다. 



## 복제 캐시 대 분산 캐시

공간 기반 아키텍처는 대부분 2가지 캐시 방법을 사용한다.

1. 복제 캐시
   * 각 처리 장치에 이름이 동일한 캐시를 사용하는 방법
   * 모든 처리 장치 간에 동기화되는 자체 인메모리 데이터 그리드를 갖고 있음
   * 한 처리 장치에서 캐시가 업데이트되면 다른 처리 장치도 새로운 데이터로 자동 업데이트되는 구조
   * 속도가 매우 빠르고 높은 수준의 내고장성을 지원하며 중앙 서버를 가지고 있는 형태가 아니기 때문에 단일 장애점이 없음
   * 하지만 특수한 상황에서 외부 컨트롤러가 반드시 필요한 캐시가 있을 수 있음
   * 또한 데이터량이 엄청나게 많거나 캐시 데이터가 너무 빈번하게 일어나면 복제 캐시를 사용하기 어려움 ( 내부 캐시의 한계 )
   * 그리고 캐시 업데이트율이 매우 높은 경우 모든 처리 장치 인스턴스에서 업데이트가 신속하게 일어나야하는데 속도를 따라잡지 못할 수 있음
2. 분산 캐시
   * 중앙 캐시를 갖고 있는 전용 외부 서버 또는 서비스가 필요
   * 처리 장치는 전용 프로토콜을 이용해 중앙 캐시 서버에 있는 데이터를 액세스 함
   * 모든 데이터가 한곳에 있으니 복제할 필요가 없어 높은 수준의 데이터 일관성을 보장
   * 하지만 원격으로 가지고 와야하기 때문에 성능이 낮고 시스템 전체 레이턴시가 증가
   * 또한 중앙 캐시 서버가 고장나면 모든게 다운되기 때문에 내고장성도 안좋음

정리하자면 복제 캐시와 분산 캐시의 차이점은 데이터의 일관성이냐, 성능 / 내고장성이냐의 문제이다. 따라서 애플리케이션 특징에 따라 적용시키면 된다.



## 니어 캐시

니어 캐시는 분산 캐시와 인메모리 데이터 그리드를 접합한 일종의 하이브리드 캐시 모델이다. 분산 캐시는 풀 백킹 캐시, 각 처리 장치에 포함된 인메모리 캐시를 프런트 캐시라 부른다.

프런트 캐시는 항상 풀 백킹 캐시와 동기화되지만 각 처리 장치에 포함된 프런트 캐시는 동일한 데이터를 공유하는 다른 처리 장치와 동기화 하지 않는다. 이는 데이터 콘텍스트를 공유하는 여러 처리 장치가 동일하지 않는 데이터를 각자의 프런트 캐시에 소유하게 될 수도 있어 권장하지 않는 모델이다.



## 아키텍처 특성 등급

| 아키텍처 특성 | 별점                           | 설명                                                         |
| ------------- | ------------------------------ | ------------------------------------------------------------ |
| 분할 유형     | 도메인 + 기술                  | 분할 유형이 다양해서 도메인 + 기술로 표현                    |
| 퀀텀 수       | 한개 또는 여러 개              | 유저 인터페이스를 설계하는 방식과 처리 장치 간의 통신하는 방법에 따라 달라짐 |
| 배포성        | :star::star::star:             |                                                              |
| 탄력성        | :star::star::star::star::star: | 인메모리 데이터 캐시를 활용하고 제약조건에 해당되는 데이터베이스를 없앴기 때문에 |
| 진화성        | :star::star::star:             |                                                              |
| 내고장성      | :star::star::star:             |                                                              |
| 모듈성        | :star::star::star:             |                                                              |
| 전체 비용     | :star::star:                   | 수십건의 데이터를 테스트하는 일이 복잡해서                   |
| 성능          | :star::star::star::star::star: | 인메모리 데이터 캐시를 활용하고 제약조건에 해당되는 데이터베이스를 없앴기 때문에 |
| 신뢰성        | :star::star::star::star:       |                                                              |
| 확장성        | :star::star::star::star::star: | 인메모리 데이터 캐시를 활용하고 제약조건에 해당되는 데이터베이스를 없앴기 때문에 |
| 단순성        | :star:                         | 주요 데이터 저장소에서 캐시를 사용하고 최종 일관성을 적용시키기 때문에 |
| 시험성        | :star:                         | 고도의 확장성과 탄력성을 시뮬레이션하는 복잡도 때문에        |
