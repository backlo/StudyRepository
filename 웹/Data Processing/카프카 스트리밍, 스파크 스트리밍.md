## Apache Kafka Stream

Kafka에 저장된 데이터를 처리하고 분석하기 위한 클라이언트 라이브러리

### 데이터를 처리하는 방법 2가지

1. Kafka -> Kafka : Kafka Stream이 aggregations, filtering 등 작업을 처리하고 데이터를 Kafka에 보낸다. 설정이 잘 되어있다면, scalability, high availability, high throughput등을 달성할수 있다.


2. Kafka -> External Systems(Database,Spark...)

## Apache Spark Streaming

![img](https://d2o2utebsixu4k.cloudfront.net/media/images/1566470507079-Apache-Kafka-Vs-Apache-Spar-4.jpg)

input data stream 으로 부터 데이터를 받아서, 일정시간 모은다음, RDD를 생성한다. 그리고 데이터를 micro-batch들로 나누고 Spark engine이 final stream을 생성한다.

## **Spark Streaming Vs Kafka Stream**

| Spark Streaming | Kafka Streams
| -- | --
실시간 데이터들이 micro batch로 쪼개진후에 처리된다.(상대적으로 느리다.) | 데이터 스트림단위로 처리된다. (상대적으로 빠르다.)
| 다른 시스템과의 데이터 호환성이 좋다. | 다른 시스템과의 데이터 호환성이 떨어진다. 
| 독립적인 processing cluster가 필요하다. | processing cluster가 필요없다.
| scaling을 위해서 설정값을 재조정해야한다. | scaling과정이 자동으로 처리된다.
| 여러 행(row)들을 처리하는데 더 적합하다.(groups,by,ml,window functions etc.) | 행들을 파싱하고, 데이터 가공에 더 적합하다.
| standalone framework | microservice로 사용될수 있고, 라이브러리 이다.


## 출처 

[https://www.knowledgehut.com/blog/big-data/kafka-vs-spark](https://www.knowledgehut.com/blog/big-data/kafka-vs-spark)

[https://www.cuelogic.com/blog/analyzing-data-streaming-using-spark-vs-kafka](https://www.cuelogic.com/blog/analyzing-data-streaming-using-spark-vs-kafka)