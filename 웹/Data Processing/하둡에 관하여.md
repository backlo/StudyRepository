# 하둡, HDFS 기본 설명



## 분산 파일 시스템

* 하나의 파일을 특정 크기로 나누어 저장
* 나눈 블록을 복제해 원복 블록과 chunk server에 알아서 저장
* 이때 복제한 블록은 언제나 다른 노드에 각각 저장
* 노드에 무슨 일을 할지 정해지고 거기에 데이터를 가지고 오는 형태로 됌
* 위에 개념이 mapreduce



## 하둡이란?

> 하둡은 대용량 데이터를 분산 처리할 수 있는 자바 기반의 오픈 소스 프레임워크
>
> 분산시스템인 HDFS에 데이터를 저장하고, MapReduce를 이용해 데이터를 처리



### 하둡의 특성

* 여러 서버에 데이터를 저장하고, 저장된 각 서버에서 동시에 데이터를 처리하는 방식
  * 병렬 분산 처리
    * 하둡은 Task 분배, 서버 고장, Task 재실행, 취합 출력 등의 문제 및 과정을 자동적으로 실행 및 해결해주는 특징을 가짐
  * 작은 용량의 데이터들 위해서 하둡을 사용하는 것은 비효율적 - 그럼 몇개?
  * 확장성 및 유 연한 데이터 구조 대응
    * 병렬 분산 처리 때문에 쉽게 서버만 늘려도 확장성이 우수
    * 뿐만 아니라 처리 시점에서 데이터를 직접 정의할 수 있기 때문에 데이터 구조에 있어 대응이 유연
* 하둡은 배치성으로 데이터를 저장하고 처리하는데 적합한 시스템
* 즉 트랜잭션이나 무결성을 보장해야하는 데이터처리에는 적합하지 않음
* Ex) 쇼핑몰
  * 회원가입, 결제진행 등 이런 요구사항은 트랜잭션이나 무결성을 보장해야함. <- 하둡으로 처리하지 않음
  * 회원이 관심있게 보는 물품들이나, 이동경로, 머무르는 시간등 배치성으로 저장되는 데이터에 적합 <- 하둡으로 처리
  * 위와 같은 예는 RDBMS를 사용할 경우 비용이 비싸기 때문에 하둡을 사용해 RDBMS와 같이 협력



### Hadoop vs RDBMS

| hadoop                                                  | RDBMS                   |
| ------------------------------------------------------- | ----------------------- |
| 정형, 비정형 데이터 구분 없이 여러대 서버에 데이터 처리 | 정형 데이터를 주로 처리 |

* 정형 데이터 : 성별, 나이 같이 의미 파익이 쉬운 데이터
* 비정형 데이터 : 텍스트, 음성, 영상 같이 의미 파악이 어려운 데이터



### Hadoop의 구성

1. 분산 저장
   * HDFS을 이용해 파일을 적당한 블록 사이즈로 나눠서 각 노드 클러스트에 저장
   * 데이터 유실의 위험이나 사람들이 많이 접근 할때의 부하 처리를 위해 각 블록의 복사본을 만들어 저장
2. 분산 처리
   * Map Reduce라는 프레임 워크를 이용해서 계산
     * Map : 함수에서 데이터 처리
     * Reduce: 함수에서 원하는 결과값을 계산



## HDFS (Hadoop Distributed File System) - namespace

> 수십 테라바이트 또는 페타바이트 이상의 대용량 파일을 분산된 서버에 저장하고, 그 저장된 데이터를 빠르게 처리할 수 있게 하는 파일 시스템

* 저사양 서버를 이용해서 스토리지를 구성할 수 있어 기존의 대용량 파일 시스템(NAS, DAS, SAN)에 비해 장점을 가짐
* 블록 구조의 파일 시스템
* 파일을 특정크기의 블록으로 나누어 분산된 서버에 저장
* 블록크기는 128M(v 2.0 이상)을 가짐
* 전역 네임스페이스를 통해 파일을 분산 저장
* **분산파일 저장 컴퍼넌트**



### HDFS의 특징

1. 대용량 데이터를 범용 서버만으로 처리가능 - 데이터 파일 크기나 개별 장비의 피일 시스템 크기에 제한이 없음
2. 용량 확장성 - 데이터가 증가하면 노드를 추가로 처리가능
3. 높은 처리량 실형 - 데이터의 부분 수정, 랜덤 접근 불가 및 큰 블록 처리 -> 고속 처리
4. 슬레이브 노드의 일부가 고장 나도 데이터 손실을 방지 가능 - 복수개의 노드에 데이터 복제 유지
5. 장애 복구 용이 - 장애에 있어서 유연한 대처
   1. 디스크 오류로 인한 데이터 저장 실패 및 유실에 같은 장애를 빠른 시간에 감지 및 대처
   2. 데이터를 저장하면 복제 데이터도 함께 저장해서 데이터 유실 방지
   3. 분산 간 서버 간 주기적인 상태 체크 (Heart Bear)
6. 스트리밍 방식의 데이터 접근
   1. HDFS에 파일 저장 및 조회를 위해 스트리밍 방식으로 설계
   2. 배치 작업과 높은 데이터 처리량을 위해서도 스트리밍 방식을 사용
7. 대용량 데이터 저장
   1. 높은 데이터 전송 대역폭
   2. 하나의 클러스터에서 수백 대의 노드를 지원
   3. 하나의 인스턴스에서 수백만개의 파일을 지원
8. 데이터 무결성
   1. HDFS는 한번 저장한 데이터를 수정할 수 없고 읽기만 가능
   2. 파일 이동, 삭제, 복사할 수 있는 인터페이스만 제공



### 네임노드와 데이터노드

* HDFS는 네임노드와 데이터노드로 구현 (마스터 - 슬레이브 관계)
* 네임노드의 핵심 기능
  * 메타데이터 관리 : 파일 속성 정보나 시스템 정보를 디스크가 아닌 메모리에서 직접 관리 (메모리 관리로 인해 응답이 빠름)
  * 데이터노드 모니터링 : 데이터노드는 네임노드에게 3초마다 ```하트비트```를 전송하기 때문에 네임노드를 이용해 데이터노드의 실행상태와 용량을 체크
    * 이때 ```하트비트```를 전송하지 않은 데이터노드가 있으면 장애서버로 판단
  * 블록 관리 : 데이터노드와 네임노드가 주기적으로 상황을 주고받으면서 용량이 부족하다면 여유가 있는 데이터노드에 불록을 이동 및 복제
  * 클라이언트 요청접수 : 클라이언트의 요청을 네임노드가 직접 접수하고 처리
    * HDFS에 파일을 저장할 경우 기존 파일의 저장여부와 권한 확인 절차를 거쳐 저장을 승인
  * 메타데이터?
    * 파일명, 디렉토리, 데이터 블록 크기, 소유자(소속 그룹), 파일 속성
    * 데이터노드와 블록 대응 정보 
      * 블록 ID와 해당 블록을 보유한 데이터노드 정보
      * 데이터노드가 하트비트를 3초 간격으로 전송 할때, 자신이 관리하는 블록 정보를 통지
      *  이를 바탕으로 전체 블록 정보를 구축 및 복제수가 충분한지 판단
    * **fsimage** : 메모리 상에 관리되어 있는 메타데이터 내의 파일 시스템 이미지
    * **edit** : 로컬 파일 시스템에 생성되는 편집로그, 메모리 상에서 관리되고 있는 파일 시스템 이미지에 적용
* 데이터노드의 핵심 기능
  * 클라이언트가 HDFS에 저장하는 파일을 로컬 디스크에 유지
  * 이때 파일은 두가지로 저장되는데 하나는 실제 저장되는 로우데이터 다른 하나는 체크섬이나 파일 생성일자 같은 메타데이터가 저장된 파일



### HDFS에 파일저장

<img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcoEG3p%2Fbtqv15KeH0D%2F3SouoMdQoLuj0XaOhSa0Q1%2Fimg.png" style = "width : 70%"/>

1. 클라이언트에서 먼저 네임노드와 통신과정을 통해 스트림(DFSOutputStream)을 생성
2. 생성된 스트림을 통해 클라이언트에서 파일을 각 데이터노드에 전송
   * 이때 저장할 파일은 패킷단위로 나뉘어서 전송
3. 파일전송이 완료되면 클라이언트에서는 네임노드에서 얻은 스트림을 close하면 남은 모든 패킷이 Flush
4. 클라이언트에서 네임노드의 complete 메소드를 호출해서 정상적으로 저장되었다면 true가 반환
5. true가 되면 파일저장이 완료



### HDFS에 파일읽기

<img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fps49x%2FbtqvZ0b6HWb%2F97QQCIyCH9uYNtq8I4ymxk%2Fimg.png" style = "width : 70%"/>

1. 클라이언트에서 네임노드의 입력스트림객체(DFSInputStream)를 통해 스트림객체를 생성
2. 생성된 스트림객체를 이용하여 기본 블록의 10배수만큼 조회
3. 클라이언트에서 스트림객체에서 블록 리더기를 다음과 같이 생성
   * 블록이 저장된 데이터노드가 같은 서버에 있다면 로컬 블록 리더기(BlockReaderLocal)를 생성
   * 블록이 저장된 데이터노드가 원격 서버에 있다면 원격 블록 리더기(RemoteBlockReader)를 생성
4. DFSInputStream은 파일 모두 읽을 때까지 블록을 조회
5. 모두 읽었다면 close를 통해 닫아줌



### 보조 네임노드(Secondary NameNode)

* 네임노드가 메타데이터를 메모리에 담고 처리하는데 만약 서버가 리부팅되면 사라질 수 있음

* 이러한 점 때문에 HDFS는 ``editslog``, ``fsimage``라는 두개의 파일을 생성 (메타데이터)

  ```
  editslog - HDFS의 모든 변경 이력을 저장한 파일
  fsimage - 메모리에 저장된 메타데이터의 파일 시스템 이미지를 저장한 파일
  ```

* 하지만 editslog가 커지면 fsimage를 만드는데 시간이 많이 소요



> 위와 같은 문제 때문에 보조 네임노드가 생김
>
> 즉 HDFS에 가지고 있던 모든 정보들이 날아갈 수 있는 위험을 방지하기 위해 사용



* 보조 네임노드는 fsimage를 갱신해줌 - 이 작업을 체크 포인트라 부름
* 따라서 보조 네임노드는 체크 포인팅 서버라 부르기도 함
* 여기서 중요한 점은 보조 네임노드는 네임노드의 백업이 아니고 단순히 fsimage를 줄여주는 역할만 함
* fsimage가 너무 커서 네임노드가 메모리에 로딩되지 못하는 경우를 예방하기 위해 사용



###  HDFS의 편리성과 신뢰성

* 파일이 어떻게 블록으로 분할 할지 의식 할 필요없이 하둡이 자동으로 진행
* 디스크 용량이 부족할 때에는 서버만 추가해주면 되기 때문에 편안
* 하나의 서버가 고장나서 블록에 접근할 수 없게 되어도 다른 서버에 동일한 블록이 저장되어 있기 때문에 데이터 소실 위험이 낮음
* HDFS 마스터 서버인 네임노드가 고장나면 GDFS 전체가 망가질 수 있는데 네임노드의 이중화를 통해 쉽게 해결 가능



## MapReduce

>  HDFS에 분산 저장된 데이터에 스트리밍 접근을 요청하여 빠르게 분산처리 하도록 고안된 프로그래밍 모델이고 지원하는 시스템

* 맵리듀스를 통해 분산, 단일 어떤 환경에서든 데이터를 병렬로 분석 가능
* 개발자는 맵리듀스 알고리즘에 맞게 분석 프로그램을 개발
* 데이터의 입출력과 병렬처리 등 기반 작업은 맵리듀스 프레임워크가 알아서 해줌
* **분산 모델 프로그램 컴포넌트**



### MapReduce 설계 요소

1. Job Client

   * 입력 데이터의 분할 방침 결정 - 처리 대상 입력 데이터를 어떻게 분할하여 병렬처리 할지 결정
   * JobTracker에게 MapReduce 의뢰
   * MapReduce 잡을 실행하기 위해 애플리케이션 HDFS에 저장
   * JobTracker로부터 진행 상태 수신
   * 사용자 단위로 MapReduce 잡을 관리 - 잡의 우선순위 변경이나 잡을 강제 종료
2. Job Tracker

   * Job 관리
     * Map Task 할당 제어
     * Map 처리 결과 파악
     * JobClient에게 잡 진행 통지
   * 리소스 관리
     * TaskTracker에게 Map이나 Reduce처리 할당
     * 처리의 주기적 실행
     * 이상 발생시 처리 재할당
     * 처리 실패 빈도가 높은 TaskTracker 블랙리스트 생성
     * TaskTracker 동작 여부 확인
     * TaskTracker 추가 / 제외
   * 잡 실행 이력 관리
3. Task Tracker
   * Child 프로세스 생성과 처리 실행
     * Child 프로세스 : map이나 reduce작업을 동작하는 자바 프로세스
     * Child 프로세스에게 jar파일이나 필요한 데이터 전달
   * Child 프로세스 상태 확인
   * JobTracker로부터 처리 중지 지시가 오면, 처리 중지를 통지
   * Map 처리 수와 Reduce 처리 수 파악
   * JobTracker에게 주기적으로 하트비트 전송
     * map 또는 reduce 동시 실행 가능한 슬롯 수, 현재 빈 슬롯 수 포함하여 전송
   * 스케쥴링



### MapReduce 구성 요소

1.  Task
   * Mapper나 Reducer가 수행하는 단위 작업
   * Map 혹은 Reduce를 실행하기 위한 정보를 가지고 있음
2. Mapper
   * 구성 : Map, Combine, Partition
   * 인풋 데이터를 가공하여 사용자가 원하는 정보를 key/value 쌍으로 변환
   * <k1, v1> -> list<k2, v2>
3. Reducer
   * 구성 : Suffle/Sort, Reduce
   * 가공된 key/value를 key 기준으로 각 리듀서로 분배
   * 사용자가 정의된 방법으로 각 key와 관련된 정보를 추출
   * <k2, list(v2)> -> list<k3, v3>



### MapReduce에 Mapping 과정, Reducing 과정

![img](https://blog.kakaocdn.net/dn/b935OH/btqv0ffNG2V/ZIerUvHZE9KhhnacmIHe70/img.jpg)

* Mapping 과정
  1. 입력파일 : HDFS에 있는 입력 파일을 분할하여 FileSplit을 생성하고 FileSplit 하나 당 Map Task 하나씩 생성
  2. Map : Map은 데이터를 읽어와서 새로운 Key-Value를 생성
  3. Combine : Map 리스트에 대한 전처리를 수행하는 과정으로 맵의 결과를 Reduce에 보내기 전에 데이터를 최소화 과정을 거침
  4. Partition: Partition은 키를 기준으로 디스크에 분할 저장하는 과정
     * 각 Partition은 키를 기준으로 정렬되고 HDFS가 아닌 맵퍼의 Local File System에 저장
     * 분할된 각 파일은 각각 다른 Reduce Task에 전달



* Reducing 과정
  1. Suffle : 여러 Mapper 있는 결과 파일을 각 Reducer에 할당하고 Reducer에 할당된 결과 파일을 Reducer의 로컬 파일 시스템으로 복사
  2. Sort : 병합정렬을 이용하여 Mapper 결과 파일을 정렬/병합하고 key로 정렬된 하나의 커다란 파일이 생성
  3. Reduce : 정렬 단계에서 생성된 파일을 처음부터 순차적으로 읽으면서 Reduce 함수를 실행

![img](https://blog.kakaocdn.net/dn/bgVeKZ/btqv0fz9irj/dZbYDkTZOwzs9ygVbWBJAK/img.png)



### MapReduce와 HDFS의 관계

![image0.jpg](https://www.dummies.com/wp-content/uploads/371842.image0.jpg)

* 하둡은 맵리듀서와 HDFS을 통해 서로 상호작용을 하면서 다음과 같이 진행

* 데이터 분할

  1. 하둡에서는 HDFS에 데이터를 저장

  2. MapReduce가 잡 처리 시 HDFS에서 데이터를 읽고, 처리 결과를 HDFS에 저장
  3. 입력데이터를 어떻게 분할하고 처리하는 지는 JobClient가 결정
  4. 하나의 Map Task가 하나의 스츨릿에서 레코드를 읽어 처리

* 데이터 지역성

  1. 데이터 -> 처리 시스템 가 아닌 처리 시스템 -> 데이터
     1. 노드간 데이터 전송량을 줄이고, 대량의 데이터를 처리하더라도 오버헤드가 크게 발생 x
     2. 잡트래커는 네임노드와 소통해 가며 태스크를 할당





```
hue : hadoop 모니터
hdfs 성능 향상하고 싶으면 파일 보관하는 것부터 고민
큰파일 한개 - 네임노드에서 하둡 클라이언트 요청 1번 -> 빠름
작은파일 여러개 - 네임노드에서 하둡 클라이언트 요청 여러번 -> 느림
* 네트워크 전송은 느림 -> 즉 로컬에서 읽을수 있게 만들기ㅈ

하둡2에서 active, standby로 spof 제거 (mapreduce2)

mapreduce1 -> mapreduce2 변경된점
* jobtracker 
	* resourceManager : 작업에 대한 스케쥴링
	* jobHistoryServer : 작업에 대한 로그, 통계 정보
	* applicationMaster : 작업안에 대한 task들이 잘되어있는지 관리


하둡 스트리밍
* 유닉스의 표준 스트림을 이용하여 자바 이외의 언어로 맵리듀스 작업 작성
* 표준 입력으로 부터 읽고 표준 출력으로 쓸 수 있는 언어라면 사용 가능
* cat <input file> | mapper | sort | reducer -> 디버깅 가능 (터미널 상에서)

스케줄러 종류
1. fifo - 선입선출
2. capacity - 퍼센트 할당
3. fair
```









## 왜 하둡을 쓰는가?
- 빅데이터 처리가능 - eg. 하루에 terabyte

- Horizontal scaling으로 계속 성능을 linear하게 향상가능

    - Disk seek time 감소

    - Hardware failure에 대해 유연함

    - Processing times이 단일 노드에 비해 빠름(병렬처리)

    - Vertical scaling으로는 위의 장점을 얻는것이 불가능하다.

## 주요 컴포넌트

- HDFS : distributed data storage

- YARN : computing cluster의 resouce를 관리해주는 시스템(어떤 리소스가 available, not available인지 트래킹), resource negotiator라 부름.

- MapReduce : 전체 클러스터에 데이터를 분산시켜서 작업을 수행하는 프로그래밍 모델, mapper와 reducer로 구성

- Pig : python이나 java코드 없이 SQL과 같은 언어로 MapReduce 작업을 가능하게 하는 High level programming api, 실제로는 내부에서 스트립트 언어로 변환됨.

- Hive : file system이 마치 SQL database처럼 보이도록 함, shell client나 ODBC와 같은 프로그램으로 접속가능, Hadoop cluster에 저장되있는 데이터를 대상으로 SQL query 실행가능

- Apache Ambari : 하둡클러스터에 대한 뷰를 제공함.(하둡 시스템에 대한 프론트엔드), Hortonworks가 사용

- Mesos : YARN과 같이 resource negotiator

- Spark :  클러스터에 있는 데이터를 reliably,효율적으로 처리 하는데 사용. YARN 또는 Mesos위에서 동작. Scale,Python, Java로 스크립트 작성가능.
SQL queries, Machine learning, Streaming data 처리 가능

- TEZ : Spark처럼 directed acyclic graph(쿼리를 실행하는데 optimal plan 생성)를 사용, 속도를 증가시키기 위해서 Hive와 보통 함께 사용된다.

- HBase : No SQL database, columnar data store(열기반 데이터 스토어), 클러스터에 있는 데이터들을 노출가능

- Apache Storm : Streaming data를 처리

- oozie : 클러스터에 작업을 스케줄링해줌. Hive->Pig->Spark->HBase 와 같이 여러 작업들 간에 스케줄링을 해줌.

- Zookeeper : 어느 노드가 살아있고 죽었는지 트래킹하는 역할

- Data Ingestion : 외부에 있는 데이터를 하둡 클러스터에 데이터를 저장하는 역할

    - Sqoop : ODBC,JDBC(legacy db)와 하둡의 커넥터 역할

    - Flume : 대용량의 web log를 클러스터에 보냄

    - Kafka

- External Data Storage

    - MySQL : Sqoop을 이용하여 클러스터에 있는 데이터를 MySQL로 저장가능

    - Cassandara, MongoDB : 실시간으로 데이터를 노출가능

- Query Processor

    - Impala

    - Hive







## Query Engine

### Apache Drill

- 비관계형 디비(또는 파일스토어)를 위한 SQL쿼리 엔진

    - Hive, MongoDB,HBase

    - flat JSON or HDFS상의 Parquet 파일, S3, Azure, 구글 클라우드, 로컬 파일 시스템

- 구글 Dremel에 기반

- 거의 SQL

### Phoneix

- 트랜젝션을 지원하는 HBase SQL Driver

- 빠르고, low-latency - OLTP 지원

- HBase에 대하여 JDBC connector 노출

- secondary indices, user-defined 함수들 지원

- MapReduce, Spark, Hive, Pig, Flume과 잘 호환

- 장점 

    - HBase에 추가 레이어를 두지 않은것처럼 매우 빠름

    - SQL을 사용할수 있기 때문에 HBase 네이티브 클라이언트를 사용보다 간편하다.

    - 더 복잡한 쿼리들을 최적화하는 작업이 가능하다. 하지만 HBase가 근본적으로 비관계형 데이터베이스라는 사실을 기억해야한다.

### Presto

- Drill과 유사

    - 많은 다른 종류의 big data 데이터 베이스들과 연결가능하고 그 데이터베이스들을 대상으로 쿼리를 실행가능

    - SQL문법과 유사함.

    - OLAP성 작업에 최적화 
    
    - analytical queries, data warehousing

- 페이스북에 의해서 개발되었고 부분적으로 유지되고 있음.

- JDBC, Command-line, Tableau 인터페이스들을 노출시킴.

- 장점

    - Drill과 달리, Cassandra connector지원

    - Facebook, Dropbox, Airbnb에서 사용

    - 단일 프레스토 쿼리로 organization내에 있는 여러 데이터 소스들로부터 데이터들을 콤바인가능

- 연결가능한 데이터베이스들

    - Cassandra

    - Hive

    - MongoDB

    - MySQL

    - Local files

    - Kafka,JMX,PostgreSQL, Redis, Accumulo







## Zookeeper

- 클러스터내에서 동기화되어야하는 정보들을 기록

  - 어느 노드가 마스터인가?
  - 작업들이 어느 워커에 할당되어있는가?
  - 현재 이용가능한 워커는 무엇인가?
- 클러스터내에 partial failure를 회복시키기 위해 사용가능
- HBase, High-Availability MapReduce, Drill, Storm 등을 통합시킬수 있는 부분

### Failure modes

- 마스터 노드가 크래시 되면, 워커노드중에 마스터노드를 선출해야함

- 워커노드가 크래시되면, 작업을 다시 재분할 해야함

- 네트워크문제가 발생하면 클러스터의 일부분이 나머지를 볼수 없게됨

### 분산시스템에서 "Primitive" 작업들수행

- 마스터 선출

  - 하나의 노드가 자신을 마스터로 등록하고, 그 정보에 락을 건다.
  - 다른 노드들은 그 락이 해제될때까지 마스터노드가 될수 없다.
  - 오직 하나의 노드만 락을 가질수 있다.

- 크래시 감지

- Group 관리 - 어떤 워커가 이용가능한가

- 테스크 할당 

### 아키텍처

![img](https://jaegukim.github.io/assets/img/post/Hadoop/zookeeper.png)  

- Zookeeper Ensemble이 Zookeeper데이터들을 복제

- 설정에 따라서  Split Brain Problem발생가능