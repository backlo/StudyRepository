## 하둡의 문제

1. HDFS에서 블록단위로 데이터를 저장하는데 기본적으로 한 블록당 크기는 64~256MB
   * 데이터크기가 블록 크기보다 상당히 작은 파일일 경우 성능이 크게 떨어짐
     * 예를 들어 15MB의 데이터를 많이 수집한다고 하면 HDFS의 블록 낭비
   * 이를 스몰 파일 문제라 부름
2. 수정, 삭제, 삽입중 삽입만 가능하다는 문제
   * 예를 들어 고객의 나이가 한살 더먹은 경우 수정이 불가
3. HDFS 활용이 중요
   * HDFS는 대용량 데이터에서 다량 조회할 때 적합 
     * 예를 들어 데이터양이 10TB밖에 안되면 관계형이나 NoSQL을 사용하는 편이 나음
   * NoSQL은 개인정보와 같이 단건 조회에서 적합 



### HBase, HDFS -> 람다 프로젝트

* 람다 프로젝트 : 하둡의 복잡한 구조와 동기화 문제 등을 해결하기 위해 나옴
  * HBase에 데이터를 256MB까지 누적시켜 대기
  * 256MB가 되면 HDFS  내에 파케이 파일로 변경함으로써 스몰 데이터 문제를 해결
  * 단건 조회는 HBase, 대량 데이터 조회는 HDFS
* 람다 프로젝트의 문제
  * 최종 사용자가 수집된 데이터를 다량 죄회할 경우 HBase에서 쌓이는 동안 데이터가 누락
  * 단건 조회로 A, a, B, b가 나오는 반면 다량 조회할 경우 A, a, B 이런식으로 나올 수 있음
  * 즉 데이터 동기화가 안되는 문제 발생
  * 그리고 HBase에 저장해 파케이로 전환하는 구조 자체가 복잡 -> 유지, 관리 어려움



## 쿠두

> HDFS, Hbase의 단점을 극복하기 위해 나온 시스템
>
> 실시간 분석 워크로드에 적합 -> 데이터 규모가 작고 지속해서 변경되는 데이터를 저장하기 좋다는 의미

1. 하둡이 가지고 있는 하드웨어 사용, 확정성, 데이터 가용성 보증 등과 같은 특성을 지원
2. 블록기반 스토리지가 아닌 NoSQL OLAP 구조의 컬럼형 스토리지 사용 ([NoSQL OLAP 참고](https://www.singlestore.com/blog/pre-modern-databases-oltp-olap-and-nosql/))
   * 삭제, 수정 지원
   * 컬럼형 스토리지로 인해 적절한 설계와 분석, 데이터 웨어하우징 워크로드에서 우월
   * 로우 기반 데이터베이스 vs 컬럼 기반 데이터베이스
     * 로우 기반은 전체 로우를 모두 참조해야한다는 단점이 있음
     * 컬럼 기반의 경우 다른 컬럼을 무시하면서 단일 컬럼 또는 해당 컬럼의 일부를 읽을 수 있음
     * 따라서 디스크의 최소 블록을 사용해 쿼리를 수행할 수 있음
     * 특정 컬럼에는 하나의 데이터 유형만 포함하기 때문에 패턴 기반 압축은 로우 기반 솔루션에 사용되는 혼합 데이터 유형을 압축하는 것보다 훨씬 효율적
3. 쿠두의 처리
   1. 테이블 : 데이터가 저장되는 공간
      1. 테이블은 스키마가 정렬된 프라이머리 키를 가짐
      2. 태블릿이라는 세그먼트로 분할
   2. 태블릿 : 테이블의 연속 세그먼트
      1. 태블릿 간 복제는 Raft 합의 알고리즘을 이용해 리더와 팔로워로 구성
      2. 태블릿은 여러 태블릿 서버에 ReplicaSet을 구성
      3. 단일 태블릿의 크기는 일반적으로 수십 GB이며 노드당 10~100 태블릿을 권장
   3. 태블릿 서버 : 태블릿을 저장하고 클라이언트에 태블릿을 제공하는 역할
      1. 쓰기 요청 처리
      2. 태블릿 리더 산출 (태블릿의 쓰기 요청은 리더, 읽기 요청은 리더와 팔로워가 수행)
   4. 마스터 : 쿠두 구성 요소의 메타 데이터를 관리
      1. 마스터 노드는 모든 태블릿, 태블릿 서버, 카탈로그 테이블과 클러스터와 관련된 기타 메타 데이터를 저장 관리하는 역할을 담당
4. 아키텍처를 단순화 시킨것으로 단건이다 다량이든 비슷한 성능을 제공
5. Sql 파서나 쉘을 제공하지 않기 때문에 임팔라가 제공하는 SQL 커넥터를 이요하면 데이터가 저장된 노드 상에서 쿼리를 바로 수행 가능
6. HDFS vs Kudu
   1. HDFS는 데이터 규모가 크고 배치용을 적합한 유형일때 유리  (콜드 데이터)
   2. Kudu 데이터의 변경이 잦고 자주 사용하는 조건에서 유리 (핫 데이터)



## 임팔라

> 아파치 하둡을 실행하는 컴퓨터 클러스터에 저장된 데이터를 위한 오픈 소스
>
> 대규모 병렬 처리 ```SQL 쿼리 엔진```

* 설계 단계부터 성능을 고려한 솔류션
* 임팔라의 확장성
  * HBase, MapReduce와 같은 별도 계층을 거치지 않고 HDFS와 직접 통신
  * 하이브처럼 하이브쿼리언어 (HiveQL)를 사용
  * 임팔라는 동일한 파일과 데이터포맷, 메타데이터, 보안 및 자원 관리 프레임워크를 사용하기 위해 하둡과 연동
  * 다양한 파일 저장소(HDFS, 쿠두, Hbase, Amazon-S3)의 데이터를 SQL을 사용해 실시간으로 분석하는 것을 지원
* 임팔라의 안정성
  * 부하가 가중되면 Implad간 리소스 경합이 일어나고 메모리가 부족한 점을 전용 코디네이터로 해결
  * 전용 코디네이터란?
    * 메타 정보를 캐싱하는 임팔라 데몬 수를 줄여 메모리 자원 사용을 최적화 하게 해주는 기능
  * 스테이트 스토어와 전용 코디네이터 사이의 메타 정보를 동기화하기 때문에 네트워크 부하를 일으키는 요소 하나를 없앨 수 있다는 장점이 있음
  * 카탈로그 서비스의 병목 현상이 5.16버전 부터 문제가 없음
    * pub/sub 구조가 pull 방식으로 변경
    * 동기화문제 해결
* 임팔라와 하이브의 차이점
  * 하이브는 데이터 접근을 위해 맵리듀스 프레임워크를 이용
  * 임팔라는 응답 시간을 최소한으로 줄이기 위해 고유의 분산 질의 엔진을 사용
    * 이 분산 질의 엔진은 클러스터 내 모든 데이터 노드에 설치
  * 따라서 임팔라와 하이브는 동일 데이터에 대한 응답 시간에 있어서 확연한 성능 차이를 보이고 있음
  * 임팔라의 좋은점
    * CPU 부하를 줄임 -> 순수 I/O bound Query 경우 하이브보다 3~4배 좋은 성능 결과 [I/O 바운드 참고](https://qastack.kr/programming/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean)
    * Query가 복잡해지면 Hive는 여러 단계의 MapReduce 작업 또는, Reduce-side 조인 작업이 필요
      * 이처럼 MapReduce에서 처리하기에 비효율적인 질의는ㄴ Impala가 7~45배 정도 좋은 성능 발휘
    * 분석할 데이터블록이 파일 캐시되어 있는 상태라면 매우 빠른 성능을 보여줌
      * Hive보다 20~90배 빠른 성능을 보여줌
* 임팔라의 구조
  * Impalad(임팔라 데몬): 하둡의 데이터 노드에 설치되어 임팔라의 실행 쿼리에 대한 계획, 스케줄링, 엔진을 관리하는 코어
    * 분산 질의 엔진 역할을 담당하는 프로세스
    * 하둡 클러스터 내 데이터 노드 위에서 질의에 대한 plan 설계와 질의 처리 작업
    * Impalad 내부 구조
      * Query Planner: 임팔라 쿼리에 대한 실행 계획을 수립
      * Query Coordinator : 임팔라의 잡 리스트 및 스케줄링 관리
      * Query Exec Engine : 임팔라의 쿼리를 최적화해서 실행하고, 쿼리 결과를 제공
  * Statestored : 분산 환경에 설치되어 있 Impalad의 설정 정보 및 서비스를 관리
    * 각 데이터 노드에서 수행되는 impalad에 대한 메타데이터를 유지하는 역할
    * 클러스터 내에 추가 또는 제거 될 때, state store 프로세스를 통해 메타데이터가 업데이트
  * Catelogd : 임팔라에서 실행된 작업들을 관리하며, 필요시 작업 이력을 제공



