# Parquet vs Avro

* 둘다 HDFS에서 사용하는 파일 포맷
* Spark, Hive등으로 구성된 빅데이터 플랫폼에서 활용하기 용이
* [자바 직렬화 참고](https://woowabros.github.io/experience/2017/10/17/java-serialize.html)



## Parquet

> 컬럼 기반(기준)의 저장 포멧

* 이는 쿼리에서 필요로 하는 데이터만을 디스크로부터 읽어들여 I/O를 최소화하는 것

* 이런 식을 보통 데이터셋의 크기를 1/3로 줄일 수 있음 -> 행 기반 포멧에 비해 파일 사이즈가 작음

* 전통적인 행 기반의 레이아웃 대신, 한 번에 컬럼 하나를 저장

* 쿼리 성능이 높음

* example

  ```
  데이터 구조              행 기반 저장방식                      컬럼 기반 저장방식(Parquet)
  |A |B |C |   |                                 |
  ----------   |                                 |
  |A1|B1|C1|   |    |A1|B1|C1|A2|B2|C2|A3|...    |    |A1|A2|A3|B1|B2|B3|C1|...
  |A2|B2|C2|   |                                 |
  |A3|B3|C3|   |                                 |
  ```

* 파케이의 장점

  * 압축률이 더 좋음 -> 유사한 데이터들이 모여있기 때문에 데이터들이 많이 모이면 압축하기 좋음 ([압축 기법 참고](https://genesis8.tistory.com/217))
  * I/O 사용률이 줄어듬 -> 데이터를 읽어 들이 때 일부 컬럼만 스캔
  * 컬럼별로 적합한 인코딩을 사용할 수 있음 -> 각 컬럼의 데이터들은 동일한 유형의 데이터를 저장 -> 인코딩 효율이 높음
  * 중첩 구조의 데이터를 저장 가능 -> 적은 오버헤드, 중첩된 필드를 다른 필드와 상관 없이 독립적으로 읽을 수 있음 -> 성능 향상
  * 파케이를 지원하는 많은 도구가 존재 -> 인메모리 표현까지 확장 -> 여러 인메모리 데이터 사용 가능(에이브로, 쓰리프트. 프로토콜)

* 파케이 파일의 속성은 파일 기록 시점에 정해짐
* 블록크기 : 스캔 효율성과 메모리 사용률 사이의 트레이드오프 관계 고려
  * 블록 크기를 더 크게 할 수록 더 많은 행을 가지므로 순차 I/O성능을 높일 수 있어 효율적인 스캔이 이루어짐
  * 하지만 개별 블록을 읽고 쓸 때 모든 데이터가 메모리에 저장되어야 하기 때문에 너무 큰 블록을 사용하는 것은 한계가 있음
  * 특히 HDFS(기본 128MB) 블록 크기보다 크면 안됌 
* 페이지 : 파케이 파일의 최소 저장 단위
  * 원하는 행을 읽기 위해서는 그행을 포함한 페이지의 압축을 해제하고 디코딩 해야 함
  * 단일 행 검색은 페이지가 작을수록 효율적 -> 원하는 값 찾기 전에 읽어야하는 값이 더 적어짐
  * 페이지가 작으면 필요한 페이지의 수가 늘어나 추가적인 메타데이터로 인해 저장 용량과 처리 시간이 증가하는 단점이 있음



## Avro

>  특정 언어에 종속되지 않은 언어 중립적 데이터 직렬화 시스템

* 하둡 Writable의 주요 단점인 언어 이식성을 해결하기 위해 만든 프로젝트
* 다른 시스템과 비슷하게 언어 독립 스키마로 기술
  * 하지만 특정 스키마를 사전에 알지 못하더라도 해당 스키마에 부합하는 데이터를 읽고 쓸 수 있음
* 에이브로의 스키마는 JSON으로 작성
  * 데이터는 바이너리 포맷으로 인코딩
* 스키마 해석 기능
  * 데이터 I/O를 하는 스키마가 서로 같지 않아도 됨
  * 스키마 변형 매커니즘 -> 새로운 필드는 적당히 무시하고 기존 데이터 작업처럼 처리 가능
* 하둡의 시퀀스 파일과 유사한 연속적 객체를 위한 객체 컨테이너 포멧을 제공
  * 에이브로 데이터 파일은 스키마가 저장된 메타데이터 섹션을 포함하고 있어 자신을 설명하는 파일이 됌
  * 압축과 분할 기능 제공
* 자료형과 스키마
  * 기본 자료형 : null, boolean, Int, long, float, double, bytes(순차8비트 부호 없는 바이트), string (순차 유니코드 문자)
  * 복합 자료형 : array, map, record(임의의 자료형으로 명명된 필드의 집합), enum(명명된 값의 집합), fixed(고정길이의 8비트 부호 없는 바이트), union



### Kafka Avro Serializer

![Image for post](https://miro.medium.com/max/2052/1*OortllduCOkKV6_s4dZ6fg.png)

* 순서
  1. Producer는 메시지를 직렬화 하기 위해 KafkaAvroSerializer를 사용
  2. kafkaAvroSerializer는 SchemaRegistryClient라는 것을 사용하여 Schema Registry에 스키마정보를 등록
  3. Schema Registry에 정상적으로 스키마가 등록되면 SchemaID를 반환
  4. KafkaAvroSerializer는 SchemaID와 메시지 본문을 포함한 데이터를 직렬화 함
     * 직렬화 구조 : Magic Byte (1byte) + SchemaID(4byte) + Data
     * 다른 Confluent Schema Registry가 앞 5바이트를 포함해서 전달하지는 않음



## Confluent Schema Registry

> 아파치 카프카와 더불어 다양한 플랫폼들을 연결시켜주는 오픈소스

* RESTful 인터페이스 사용
* 스키마를 관리하거나 조회하는 기능을 제공
* Producer, Consumer는 Apache Avro를 통해 포맷 메시지를 Kafka를 통해 전달하고 받음
* 받는 순서

![Image for post](https://miro.medium.com/max/1904/1*6crlNRdKaB7B1P82rE10UQ.png)

1. Consumer는 Kafka로 부터 바이너리 데이터를 받는데 이 데이터에는 스키마 ID가 포함되어 있음
2. 해당 ID를 가지고 로컬 캐시 혹은 Confluent Schema Registry에서 스키마 정보를 탐색하여 가져오고, 사크마 정보를 사용해 역직렬화를 함



### 스키마 진화 전략

* Backward : 새로운 스키마로 이전 데이터를 읽는 것
  * 새로운 스키마 필드에 해당 필드에 default value가 없으면 오류 발생
  * 새로 추가되는 필드에 default value를 지정할 때에만 스키마 등록이 허용
  * Confluent Schema Registry는 기본적으로 Backward로 동작
* Forward : 이전 스키마에서 새로운 데이터를 읽는 것
  * 새로운 스키마에서 특정 필드가 삭제된다면, 해당 필드는 이전 스키마에 default value를 가지고 있어야함
* Full : Backward, Forward를 모두 만족하는 것 (추천)
* None : Backward, Forward 둘다 만족하지 못하는 것



### Schema Registry 장단점

* 장점
  * 안전성 - 스키마 레지스터리가 잘못되면 카프카에 메시지를 전달하지 않음
  * 카프카에 전달되는 바이너리 데이터에 스키마 정보가 포함되어 있지 않기 때문에 상대적으로 적은 용량의 데이터가 전달
  * 따라서 json등과 비교하여 카프카 시스템에서 더 적은 공간을 차지하고 네트워크 대역폭도 절약할 수 있음
* 단점
  * 스키마 레지스터리에서 장애가 발생하면 정상적으로 메시지 전달이 x
  * 카프카와 비교했을 때 운영포인트가 증가
  * Avro 포맷은 Json과 비교했을 때 직렬화과정에서 퍼포먼스가 조금 떨어짐 (하지만 크게 떨어지지는 않음











## Apache Avro vs Parquet

**Apache Avro** 는 remote procedure call과 data serialization framework 이고 Apache Hadoop project중 개발되었다. type,protocol을 정의하는데 JSON을 사용하고 compact binary format으로 데이터를 직렬화한다.

- **row based format**이기 때문에, 모든필드들이 접근될필요가 있을때 사용하는것이 좋다.
- 파일들은 block compression을 지원하고 쪼개질수있다.
- streaming data로 쓰여질수있다.(eg. Apache Flink)
- write intensive operation에 적합하다.

**Apache Parquet**, 반면에 Apache Hadoop ecosystem의 open-source column oriented 데이터 스토리지이다. RCFile,ORC와 같은 다른 columnar-storage file format과 유사하다. bulk로 복잡한 데이터를 처리하는 작업에 있어서 효율적인 데이터 압축과 인코딩 스키마를 제공한다.

- **column based format** 이기 때문에, 특정 필드를 접근할 필요가 있을때 사용하는것이 좋다.
- 각각의 데이터 파일은 일련의 row들에 대한 값들을 포함한다.
- 블록들이 처리가 될때까지 기다려야 하므로, streaming data로 쓰여질수 없다. 
- data exploration에 적합하다 - read intensive, complex or analytical querying, low latency data

두가지 포맷이 다음을 지원한다.

- Schema evolution
- Nested dataset

